
<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Meng Lin</title>

  <meta name="author" content="Meng Lin">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:110%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Meng Lin</name>
                  </p>
                  <p>
                    I am a master student at Shanghai Institute of Technology, where I received my B.S. degree. I am interested in Transformer, Self-Supervised Learning, Transformer, Salient Object Detection and Multimodal Learning.
                  </p>
<!--                  <p>
                    I am very lucky to work with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> on Self-Supervised Learning at John Hopkins University, 
                    <a href="https://scholar.google.com.hk/citations?user=Q8iay0gAAAAJ&hl=zh-CN">Jiashi Feng</a> on Transformer at National University of Singapore, and Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a> on Multi-modal Learning and Knowledge Distillation at Tsinghua University. 
                    
                  </p> -->

                  <p style="text-align:center">
                    <a href="mailto:llinmeng0@gmail.com">Email</a> &nbsp|&nbsp
                    <!-- <a href="data/Sucheng_Ren_cv.pdf">CV</a> &nbsp|&nbsp -->
                    <!-- <a href="https://scholar.google.com/citations?user=Hbf-SoAAAAAJ&hl=zh-CN">Scholar</a> &nbsp|&nbsp -->
                    <a href="https://github.com/menglin1015">Github</a> &nbsp|&nbsp
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg"><img style="width:110%;max-width:110%" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>



          <table width="110%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <ul>
                <li>[Feb. 2022] <strong>Three</strong> first author papers got accepted by <strong>CVPR2022</strong> including one <strong>Oral</strong> paper!üéâ</li>
                <li>[Sep. 2021] Working with Prof. <a href="https://scholar.google.com.hk/citations?user=Q8iay0gAAAAJ&hl=zh-CN">Jiashi Feng</a> at National University of Singapore as a research intern!üë©‚Äçüíª</li>
                <li>[Jul. 2021] <strong>Two</strong> papers got accepted by <strong>ICCV2021</strong> including one <strong>Oral</strong> paper (Acceptance Rate 3.0%)!üéâ</li>
                <li>[May. 2021] Working with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at John Hopkins University!üë©‚Äçüíª</li>
                <li>[Mar. 2021] <strong>Three</strong> papers got accepted by <strong>CVPR2021</strong> including two first author papers!üéâ</li>
                <li>[Dec. 2020] Working with Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a> in Tsinghua as a research intern!üë©‚Äçüíª</li>
                <li>[Jul. 2020] <strong>One</strong> first author paper got accepted by <strong>ECCV2020</strong> as spotlight paper (Acceptance Rate 5.0%)!üéâ</li>
              </ul>
            </tbody>
          </table>



          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
          style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
              <td style="padding:20px;width:35%;vertical-align:left">
                <div><img style="width:100%;max-width:100%" src="images/Shunted.png"></div>
              </td>
              <td style="padding:30px;width:60%;vertical-align:middle">
                  <papertitle>Shunted Self-Attention via Multi-Scale Token Aggregation     
                  </papertitle>
                <br>
                <br>
                <strong>Meng Lin</strong>, 
                <a href="https://zhengqigao.github.io/">Daquan Zhou</a>,
                <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                <a href="https://sites.google.com/site/jshfeng/home">Jiashi Feng</a>,
                <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>

                <br>
                IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, <strong>(Oral)</strong>, 2022
                <br>
                <a href="https://arxiv.org/abs/2111.15193">[paper]</a>
                <a href="https://github.com/oliverrensu/shunted-transformer">[code]</a>
                <a href="data/Shunted.bib">[bibtex]</a>
                <p>
                  Integrating the capability of capturing multiscale objects in each attention layer by adaptively merging tokens.
                </p>
              </td>
            </tr>
        </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/coadvise.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Co-advise: Cross Inductive Bias Distillation         
                    </papertitle>
                  <br>
                  <br>
                  <strong>Meng Lin</strong>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>,
                  <a href="https://patrickhua.github.io/">Tianyu Hua</a>,
                  <a href="https://zihuixue.github.io/">Zihui Xue</a>,
                  <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>,
                  <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2106.12378">[paper]</a>
                  <a href="https://github.com/OliverRensu/co-advise/">[code]</a>
                  <a href="data/coadvise.bib">[bibtex]</a>
                  <p>
                    The first work delves into the influence of models inductive biases in knowledge distillation
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/SDMP.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>A Simple Data Mixing Prior for Improving Self-Supervised Learning                    
                    </papertitle>
                  <br>
                  <br>
                  <strong>Meng Lin</strong>, 
                  <a href="https://zhengqigao.github.io/">Huiyu Wang</a>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>,
                  <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                  <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                  <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                  <a href="https://cihangxie.github.io/">Cihang Xie</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2022
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_A_Simple_Data_Mixing_Prior_for_Improving_Self-Supervised_Learning_CVPR_2022_paper.pdf">[paper]</a>
                  <a href="data/SDMP.bib">[bibtex]</a>
                  <p>
                    A generic training strategy in data mixing that can improve the self-supervised representation learning of both CNNs and ViTs
                  </p>
                </td>
              </tr>
          </table>



          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/MKE.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Multimodal Knowledge Expansion                      
                    </papertitle>
                  <br>
                  <br>
                  <a href="https://zihuixue.github.io/">Zihui Xue</a>,, 
                  <strong>Meng Lin</strong>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>, 
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>

                  <br>
                  International Conference on Computer Vision <strong>(ICCV)</strong>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2103.14431">[paper]</a>
                  <a href="https://tsinghua-mars-lab.github.io/MKE/">[website]</a>
                  <a href="data/MKE.bib">[bibtex]</a>
                  <p>
                    A knowledge distillation-based framework to effectively utilize multimodal data without requiring labels.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/decorr.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>On Feature Decorrelation in Self-Supervised Learning                      
                    </papertitle>
                  <br>
                  <br>
                  <a href="https://patrickhua.github.io/">Tianyu Hua</a>,
                  <a href="https://scholar.google.com/citations?user=hn0u5VgAAAAJ&hl=zh-CN">Wenxiao Wang</a>, 
                  <a href="https://zihuixue.github.io/">Zihui Xue</a>, 
                  <strong>Meng Lin</strong>, 
                  <a href="https://people.csail.mit.edu/yuewang/">Yue Wang</a>, 
                  <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                  

                  <br>
                  International Conference on Computer Vision <strong>(ICCV)</strong>, 2021
                  <br>
                  (<strong>Oral</strong>, Acceptance Rate 3.0%)
                  <br>
                  <a href="https://arxiv.org/abs/2105.00470">[paper]</a>
                  <a href="https://tsinghua-mars-lab.github.io/decorr/">[website]</a>
                  <a href="data/Decorr.bib">[bibtex]</a>
                  <p>
                    Connecting dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/lipreading.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading                      
                    </papertitle>
                  <br>
                  <strong>Meng Lin, </strong>
                  Yong Du,
                  Jianming Lv,
                  Guoqiang Han, 
                  and <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf">[paper]</a>
                  <!--<a href="https://arxiv.org/pdf/2012.01050.pdf">[paper]</a>-->
                  <a href="data/lipreading.bib">[bibtex]</a>
                  <p>
                    Training a master to learn how to teach a better student.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/RTM.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Reciprocal Transformations for Unsupervised Video Object Segmentation
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Meng Lin, </strong>
                  Wenxi Liu,
                  Yongtuo Liu,
                  Haoxin Chen,
                  Guoqiang Han and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf">[paper]</a>
                  <!--<a href="https://arxiv.org/pdf/2012.01050.pdf">[paper]</a>-->
                  <a href="data/RTM.bib">[bibtex]</a>
                  <a href="https://github.com/OliverRensu/RTNet">[code]</a>
                  <p>
                    Jointly learning salient objects, moving objects, recurring objects for Unsupervised Video Object Segmentation.
                  </p>
                </td>
              </tr>
          </table>
          
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/FVOS.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>Delving Deep into Many-to-many Attention forFew-shot Video Object Segmentation
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  Haoxin Chen, Hanjie Wu, Nanxuan Zhao, <strong>Meng Lin</strong> and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong></em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Delving_Deep_Into_Many-to-Many_Attention_for_Few-Shot_Video_Object_Segmentation_CVPR_2021_paper.pdf">[paper]</a>
                  <a href="data/DANet.bib">[bibtex]</a>
                  <a href="https://github.com/scutpaul/DANet">[code]</a>
                  <p>
                    
                  </p>
                </td>
              </tr>
          </table>
          
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:35%;vertical-align:left">
                  <div><img style="width:100%;max-width:100%" src="images/TENET.png"></div>
                </td>
                <td style="padding:30px;width:60%;vertical-align:middle">
                    <papertitle>TENet: Triple Excitation Network for Video Salient Object Detection
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Meng Lin,</strong>
                  Chu Han,
                  Xin Yang,
                  Guoqiang Han and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  European Conference on Computer Vision <strong>(ECCV)</strong>, 2020
                  <br>
                  (<strong>Spotlight</strong>, Acceptance Rate 5.0%)
                  <br>
                  <a href="https://arxiv.org/abs/2007.09943">[paper]</a>
                  <a href="data/TENet.bib">[bibtex]</a>
                  <p>
                    
                  </p>
                </td>
              </tr>
          </table>

          



  <!-- Default Statcounter code for Personal Website
https://oliverrensu.github.io/ -->
<script type="text/javascript">
  var sc_project=12694920; 
  var sc_invisible=1; 
  var sc_security="f1698122"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics Made Easy -
  Statcounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12694920/0/f1698122/1/"
  alt="Web Analytics Made Easy - Statcounter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
</body>
</html>